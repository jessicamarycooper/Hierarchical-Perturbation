

\documentclass{article} % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS

\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{amsfonts}
\usepackage[switch]{lineno}
\usepackage{authblk}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\pdfinfo{   
/Title (Explaining Black Box Models Quickly with Hierarchical Perturbation)
/Author (Jessica Cooper)

} 
\begin{document}
\title{Explaining Black Box Models Quickly with Hierarchical Perturbation}
\author {Jessica Cooper}
\affil{University of St Andrews}
\date{}
\maketitle

\begin{abstract}

\end{abstract}

We propose a very fast way of generating robust saliency maps for black-box models.

\section{Introduction}

As artificial intelligence is applied to increasingly high-stakes domains, deep learning researchers and practitioners are placing more and more emphasis upon techniques that allow us to understand and explain the predictions made by the complex models we build and use. Having tools available to validate that, for example, a biopsy is classed as malignant due to cell morphology, rather than due to a smudge on the slide, is crucial if we are to see safe, widespread adoption of powerful AI techniques -- and as such we have seen research into "explainable AI" (XAI) boom\cite{Fouladgar2020-hz, Vilone2020-iu, Barredo_Arrieta2020-yo, Das2020-wm}.

Explanations generated by XAI algorithms could be global (what has the model learned from the dataset?) or local (what has caused the model to produce this particular output for an individual sample?). We are concerned with local explanation -- also known as \emph{attribution} -- that is, identifying which parts of a given input are more or less important in determining a network's output. One way of doing this is in a human-interpretable way is by generating a saliency map -- a heat-map assigning colour or brightness to regions of the input according to how much they contributed to the output. Saliency visualisation of this kind is widely used in machine learning, particularly in image classification tasks that often require the use of large and complex neural networks which are troublesome to interpret otherwise.

Existing saliency mapping methods fall into two broad categories - white-box, which depend on the structure and internal state of the trained model, and black-box, which do not. White-box methods are typically much more efficient, but can only be used when the trained architecture of the model in question is both accessible, and of a specific type (normally a deep convolutional neural network), which limits their application. In contrast, black-box methods are entirely model-agnostic - they work by perturbing the input and inspecting the change in model output to determine the perturbed region's importance, irrespective of the internal state of the model. However, existing techniques of this kind are slow, as they build up this empirical estimation of region importance through many iterations.

We suspect there are a number of important AI applications where explicability is very important, but the individual data samples are too big for existing black-box saliency methods to be computationally feasible, and the network architecture is inaccessible or unsuitable for the application of faster gradient- and activation-based explanatory methods. These might be things like healthcare triage using ensemble or hybrid architectures taking both tabular clinical data and high resolution medical imaging; legal tasks using large textual or tabular datasets, or autonomous vehicle decision-making combining video, sensor and time-series input, to name a few~\cite{Tuckey2019-uj, Manikandan2020-yw}.

We therefore contribute a new model-agnostic saliency mapping method suitable for these use cases and evaluate it on the commonly-used MSCOCO and VOC validation and tests datasets using the pointing game benchmark~\cite{} and the causal insertion/deletion metrics ~\cite{Petsiuk2018-wx}. We show that our method is an order of magnitude faster than existing black-box methods in the literature, while maintaining comparable performance on these benchmarks. To our knowledge this is the first comparative ranking of saliency-mapping methods by speed. We also propose the use of a new perturbation method -- the local mean -- and show that it improves performance for perturbation based methods.

\section{Related Work}

\subsection{White-Box Methods}

White-box methods typically leverage the convolutional network architecture to visualise explanations by using gradients, activations, or some combination of the two~\cite{Selvaraju2016-vq, Simonyan2013-rl, Zhang2016-vm, Mahendran2016-hf, Zhang2016-vm}. They are efficient, but some have proven unreliable -- no better than edge detection in some cases~\cite{Adebayo2018-om, Schneider2020-jx} -- which makes them unsuitable for high-stakes tasks in robotics, medicine or similar. As raised in~\cite{Fong2019-vk}, they are not grounded in \emph{what} makes some region or input more or less salient - rather, their explanatory power is assessed a-posteriori. At present, methods of this kind are only applicable to a limited subset of architectures and only when the trained model's internal state is accessible. In this work, we focus on black-box methods, which have the substantial benefit of model-agnosticism and require access only to the input and output. 

\subsection{Black-Box Methods}

Fong et al~\cite{Fong2017-jg} propose Extremal Perturbation, using gradient descent to learn a perturbation mask which minimises (or conversely, maximises) the prediction of the target class. This method produces binary segmentations, which look neat, but obscure any difference in feature salience \emph{within the broadly salient region}. Extremal Perturbation also requires the selection of many hyperparameters (learning rate, number of iterations, mask upsampling factor, degree of blur, degree of jitter, et cetera) which are chosen empirically and may not generalise to novel models or datasets - again, raising the lengthy and uncertain prospect of manual tuning to generate informative, interpretable saliency maps. The published settings for PASCAL VOC and COCO result in excellent performance, but take an extremely long time in comparison to other methods. 

Other techniques consist of training a second model using a saliency criterion to generate attribution maps directly from the input sample~\cite{Ribeiro2016-xl, Dabkowski2017-td}, an approach which is very fast once the saliency model has been trained - however, applying this approach to a new dataset and predictive model would require retraining the saliency model too, which may not be trivial. More importantly, explanations generated in this way are by their nature fundamentally divorced from the model in question and may not be accurate characterisations of what it has learned. 

Other black-box methods work by iteratively perturbing regions of the input sample~\cite{Petsiuk2018-wx, Zeiler2013-lm}, and using the sensitivity of the model output to these perturbations to generate a saliency map. These methods have the nice property of direct interpretability (i.e. a perturbation in the input can be directly mapped to a change in the output), but are computationally expensive. Typically they also require heuristic parameter selection (for example, selecting the size of the perturbation kernel or number of masks to generate) to produce informative visualisations, which may necessitate many trials, and thereby also prove costly. Zeiler et al~\cite{Zeiler2013-lm} outline an early form of this approach, in which a perturbation kernel of fixed size and is iteratively applied to the input, and the difference in output at each kernel location is collated to form a saliency map. This is intuitive, but very time consuming, and reliant on running potentially many trials with different kernel dimensions to generate informative visualisations, since it is impossible to know the scale of the most salient features ahead of time. 

RISE~\cite{Petsiuk2018-wx} is based on the same perturbation technique, and works by generating some number of low resolution random binary masks, upsampling them using bilinear interpolation, using them to mask the input, and weighting each mask by the model's output for the correspondingly masked input (the perturbation in this case being the dimming of the input). The weighted masks are then summed and normalised, producing a saliency map. The dimensions of the low resolution and upsampled masks, and the number of masks used, are chosen empirically (8000 masks were used for ResNet50). The fact that the masks are randomly generated means that RISE must always use a large enough number of masks to counteract this randomness, in order to avoid biasing the saliency map with unevenly distributed perturbations and noise, especially when there are several salient regions of varying sizes contained in the input. The larger the input dimension, the larger this number must be. This concern can be alleviated by decreasing the resolution of the initial low-res binary mask before interpolation, thereby decreasing the number of possible masks - however, the lower resolution this mask is, the coarser the final saliency map will be, making RISE prohibitively expensive for high resolution data which demands high resolution saliency. These limitations are explicitly mentioned in the original  publication~\cite{Petsiuk2018-wx}, which calls for future work to address this by intelligently selecting a smaller number of masks - as we do here.

To address the limitations outlined above, we propose a novel perturbation algorithm which identifies salient regions regardless of scale, removes the need for heuristic parameter selection, and dramatically reduces computational cost compared to the fastest existing direct perturbation methods whilst maintaining accurate saliency detection. Note that, although we benchmark the proposed method on image data, our approach can be applied to data of arbitrary dimension, including text, video, time-series, et cetera. We compare our approach to other model-agnostic saliency mapping methods on the well established pointing game benchmark (although we also discuss possible downfalls of this metric), and the causal insertion/deletion metric~\cite{Zeiler2013-lm}.

\section{Proposed Method}

Our method focuses on more salient regions with increasing resolution whilst ignoring regions which do not not change the model’s output. This is a natural extension of iterative occlusion~\cite{Zeiler2013-lm} and the random masking of RISE~\cite{Petsiuk2018-wx} described in the previous section, in which we take the same principles of empirical salience estimation, but apply them in a more directed fashion to minimise computational cost and thereby make black-box saliency mapping a realistic prospect for large samples or datasets. Our key insight is that a large amount of superfluous computation is performed when regions that have little effect on the model output are iteratively perturbed, or when random perturbed region selection results in spatially similar or overlapping regions. By avoiding this unnecessary cost through salience thresholding we are able to perform black-box saliency mapping an order of magnitude faster than existing methods. 

Let $f:X \rightarrow \mathbb{R}$ be our model, which takes $X$, a matrix of size $3  \times H \times W$ (in our case a three channel colour image with variable height and width), and returns a vector of size $C$, where $C$ is equal to the total number of classes and $C_{t}$ is the (non-normalised) confidence prediction for our target class $t$, which we wish to attribute to some region of $X$.

We instantiate $S$ as our saliency map, initially a zero matrix of size $H \times W$, and populate it as follows:

We let our mask placeholder $M:\Lambda \rightarrow \{0,1\}$ be a matrix of size $d \times d$, where initially $d = 8$.

We then use a step function $t(S,M) \rightarrow \{0,1\}$ using the mid-range of the current saliency map as a threshold to identify regions of high salience for higher-resolution mapping thus (where $\circ$ denotes element-wise multiplication):

\begin{equation}
    t(S,M) = \left\{\begin{matrix}
    1, & if & \max{(S\circ M)}\geq \min{S} + \frac{(\max{S} - \min{S})}{2}\\
    0, & otherwise &
    \end{matrix}\right.
\end{equation}

Note that the first time this operation is applied, the saliency map consists only of zeros, and so $t(S,M) = 1$ in every case.

For every possible instance of $M$ where $i,j < d$ and $\sum{M_{i:i+2,j:j+2}} = 4$ (so all possible masks M which contain a $2 \times 2$ region of ones, with all other elements set to zero), and $t(S,M) = 1$, we upsample $M$ to size $H \times W$ using proximal interpolation, and update our saliency map $S$ such that: 

\begin{equation}
    S = S + ReLU(f(X)-f(X \circ |M-1|))_{t} \circ |M-1|
\end{equation}

We use ReLU so that when using the mid-range we are thresholding only with respect to perturbations which decreased the confidence of the target class, and therefore only highlight regions which, when available to the model, have a positive influence on the target class confidence. 

We then double $d$, and repeat the above while $d < \frac{\min{(H,W)}}{4}$.

\subsection{Saliency Thresholding}

\subsection{Perturbation Method}

Occlusion ('fade')
Note that during dataset transformation inputs are normalised with a mean of zero, so replacing pixels with zero is equivalent to replacing with mean of dataset
No information in occluded region, but introduces rectangular artefact. I don't see why this should be a problem, though. 

Perturbation ('blur')

Smooth blurring widely used in saliency mapping. Seems to me like this is because it makes sense to humans - how do we remove an object from an image without making too big of a change? But I'm a bit unconvinced. Seems like blurriness could be more confounding than occlusion. At least in occlusion, only the edges of the occluded region are weird - the inside contains no info and so is basically ignored (AM I RIGHT TO THINK THIS OR DID I MAKE IT UP?) whereas if the entire region is blurred, we just get confusion propagated. 

Note also, I want to do some stuff with in-painting as a perturbation method. 

Perturbation ('local mean')

We propose a new perturbation method - the local mean. This 


\subsection{Parameters}

Starting number of cells?

\section{Experiments}

%voc:
    % thresholding strategies
    % interpolation choices
    % different starting number of cells
    % insertion/deletion metric
    % perturbation strategy
    


% mscoco & voc:
    % accuracy, speed]
    
%RISE produces saliency maps for all classes at once for any given image - however even averaged over all classes rather than all images, the proposed method is 10x faster.

\section{Discussion}

As raised in~\cite{Zeiler2013-lm} we believe the pointing game is flawed - it relies on an assumption that if a model is good, it is good because it learns the same features that a human would, and so a good saliency map would highlight those same features. This means that it rewards maps that encode what a human being would consider to be salient, rather than what is in fact salient to the model which we hope to understand. For example, a model classifying a fridge may also identify a microwave as equally salient, as it provides important contextual information which may contribute a large part of the 'fridge' confidence. A saliency method faithfully identifying this would be penalised for it. We \emph{cannot} assume that even an 100\% accurate model has learned to use the same features that we would to make a prediction, and so must beware of biasing the development of saliency methods to confirm our assumptions, rather than provide accurate characterisations of what the model is really doing.

One of the strengths of the proposed method is that it is content aware in a way that existing perturbation-based saliency mapping algorithms are not. \cite{Petsiuk2018-hc, Fong2019-vk, Fong2017-jg, zeilferg} and others require a pre-specified number of iterations - whether that is epochs, number of random masks generated, or occlusion kernel size and step - which fix the amount of computation required for an input of given size irrespective of the proportion of the input that is actually salient. It is also impossible to know ahead of time what the optimal value for these parameters might be in order to trade-off accuracy and efficiency, and finding the optimal parameters (for one input sample, or across an entire dataset) may require many trials. The proposed method, by contrast, continually disregards regions which have little affect on the model output, and by so doing so inherently limits the amount of computation required.

Unlike RISE~\cite{Petsiuk}, which generates masks randomly (it is worth noting that that at large number of masks the effect of this is minimal, though we imagine that this is not the case when the number of masks is small) and methods which learn secondary models through gradient descent such as~\cite{Fong2019-vk,Ribeiro2016-xl, Dabkowski2017-td}, the proposed method contains no random elements. However, the proposed method is not able to capture instances where the salience of two spatially distinct features in combination is greater than the sum of each feature individually (i.e. if perturbing the microwave and oven \emph{at the same time} changes the prediction more than the difference when perturbing the oven \emph{plus} the difference when perturbing the microwave), since we perturb only one locality at a time - this unlike RISE, which with a high enough number of random masks will presumably capture this phenomenon (although with an uncertain degree of precision), and unlike Extremal Perturbation, which by design will capture ant combination of features if trained for long enough (albeit at significant time cost, as our experiments show). This theoretical limitation does not seem to affect our method's performance on the benchmarks examined here.

... what constitutes an interpretable saliency map is incredibly prone to human bias, and so methods like this which invite manual tuning per model, dataset or even per sample, can easily result in maps that are unintentionally optimised to look convincing to humans, but which are not accurate characterisations of what the model has actually learned. Many methods explicitly optimise for smooth, high contrast maps, and thereby fail to capture unexpected learned features or saliency variations across the entire input. Saliency methods shouldn't be optimised for prettiness, rather for an explanation as accurate as we are able to achieve whilst remaining interpretable. 

A model's architecture, parameters, and hyperparameters form a perfectly accurate explanation for its behaviour, but are uninterpretable to humans~\cite{Fong2019-vk}. If we neatly outline a random cluster of pixels in an image and label them 'most salient', our drawing would be perfectly interpretable, but inaccurate. We hope with this work we have avoided much of the temptation to optimise for prettiness. In summary, we contribute a fast and robust saliency mapping technique which will prove useful to the interested community requiring speedy explanations of black-box predictions.

\bibliography{refs}
\end{document}
